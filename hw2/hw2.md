# CS 601.471/671: Self-supervised Models
## Homework 2: Counting-based Language Models, Gradient, and MLP.

### Overview
In this homework, we will:
- implement n-gram language models
- implement gradient calculation and backpropagation on the model you built for hw1
- extend your hw1 model to a multi-layer perceptron (MLP) for sentiment classification

### Setup
create a new environment for this homework:
```
conda create -n ssm_hw2 python=3.10.13
```

And install the required packages:
```
cd hw2
conda activate ssm_hw2
pip install -r requirements.txt
```

### Run the code
Once you have finished each part of the homework, uncomment the corresponding lines in `main.py` to run it.

### Submission
Please follow the instructions in the homework pdf.
